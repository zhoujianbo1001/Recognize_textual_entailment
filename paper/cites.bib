@phdthesis{MacCartney2009,
author = {MacCartney, Bill},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacCartney - 2009 - Natural Language Inference.pdf:pdf},
mendeley-groups = {Recognizing Textual Entailment},
pages = {179},
title = {{Natural Language Inference}},
url = {https://nlp.stanford.edu/{~}wcmac/papers/nli-diss.pdf},
year = {2009}
}

// 深度学习在nlp中的应用
@inproceedings{Kneser1995Improved,
  title={Improved Backing-off for N-gram Language Modeling},
  author={Kneser, Reinhard and Ney, Hermann},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  pages={181-184 vol.1},
  year={1995},
}
@INPROCEEDINGS{nnlm:2001:nips,
    author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal},
     title = {A Neural Probabilistic Language Model},
      year = {2001},
       url = {http://www.iro.umontreal.ca/~lisa/pointeurs/nips00_lm.ps},
  crossref = {NIPS13},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that
has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.},
topics={Markov,Unsupervised,Language},cat={C},
}
@proceedings{DBLP:conf/interspeech/2010,
  editor    = {Takao Kobayashi and
               Keikichi Hirose and
               Satoshi Nakamura},
  title     = {{INTERSPEECH} 2010, 11th Annual Conference of the International Speech
               Communication Association, Makuhari, Chiba, Japan, September 26-30,
               2010},
  publisher = {{ISCA}},
  year      = {2010},
  timestamp = {Thu, 24 Aug 2017 11:26:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/interspeech/2010},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/Graves13,
  author    = {Alex Graves},
  title     = {Generating Sequences With Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1308.0850},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.0850},
  archivePrefix = {arXiv},
  eprint    = {1308.0850},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Graves13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/MelisDB17,
  author    = {G{\'{a}}bor Melis and
               Chris Dyer and
               Phil Blunsom},
  title     = {On the State of the Art of Evaluation in Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/1707.05589},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.05589},
  archivePrefix = {arXiv},
  eprint    = {1707.05589},
  timestamp = {Mon, 13 Aug 2018 16:47:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MelisDB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Mikolov2013_1,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Mikolov2013_2,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}
@article{Pennington2014,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
file = {:home/mr-zhou/文档/glove.pdf:pdf},
mendeley-groups = {Natural Language Process/Models},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@inproceedings{DBLP:conf/nips/LevyG14,
  author    = {Omer Levy and
               Yoav Goldberg},
  title     = {Neural Word Embedding as Implicit Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
               on Neural Information Processing Systems 2014, December 8-13 2014,
               Montreal, Quebec, Canada},
  pages     = {2177--2185},
  year      = {2014},
  crossref  = {DBLP:conf/nips/2014},
  url       = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
  timestamp = {Wed, 10 Dec 2014 21:34:12 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/nips/LevyG14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/tacl/LevyGD15,
  author    = {Omer Levy and
               Yoav Goldberg and
               Ido Dagan},
  title     = {Improving Distributional Similarity with Lessons Learned from Word
               Embeddings},
  journal   = {{TACL}},
  volume    = {3},
  pages     = {211--225},
  year      = {2015},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
  timestamp = {Thu, 28 May 2015 10:59:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tacl/LevyGD15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/KirosZSZTUF15,
  author    = {Ryan Kiros and
               Yukun Zhu and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Antonio Torralba and
               Raquel Urtasun and
               Sanja Fidler},
  title     = {Skip-Thought Vectors},
  journal   = {CoRR},
  volume    = {abs/1506.06726},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.06726},
  archivePrefix = {arXiv},
  eprint    = {1506.06726},
  timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KirosZSZTUF15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  archivePrefix = {arXiv},
  eprint    = {1409.0473},
  timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  archivePrefix = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SutskeverVL14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  archivePrefix = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Mon, 13 Aug 2018 16:46:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WuSCLNMKCGMKSJL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Kalchbrenner2016Neural,
  title={Neural Machine Translation in Linear Time},
  author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron Van Den and Graves, Alex and Kavukcuoglu, Koray},
  year={2016},
}
@article{Gehring2017Convolutional,
  title={Convolutional Sequence to Sequence Learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year={2017},
}

@article{Vaswani2017Attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year={2017},
}

@article{Chen2018The,
  title={The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation},
  author={Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng},
  year={2018},
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@article{Chen2016,
abstract = {Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6{\%} on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.},
archivePrefix = {arXiv},
arxivId = {1609.06038},
author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhenhua and Wei, Si and Jiang, Hui and Inkpen, Diana},
doi = {10.1016/0923-2494(90)90036-X},
eprint = {1609.06038},
file = {:home/mr-zhou/文档/P17-1152.pdf:pdf},
issn = {09232494},
journal = {Research in Immunology},
mendeley-groups = {Recognizing Textual Entailment},
month = {sep},
number = {4},
pages = {431--440},
title = {{Enhanced LSTM for Natural Language Inference}},
url = {http://arxiv.org/abs/1609.06038},
volume = {141},
year = {2016}
}

@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:home/mr-zhou/文档/1802.05365.pdf:pdf},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}

@article{Gong2017,
abstract = {Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20{\%} error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.},
archivePrefix = {arXiv},
arxivId = {1709.04348},
author = {Gong, Yichen and Luo, Heng and Zhang, Jian},
eprint = {1709.04348},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong, Luo, Zhang - 2017 - Natural Language Inference over Interaction Space.pdf:pdf},
mendeley-groups = {Recognizing Textual Entailment},
month = {sep},
number = {2017},
pages = {1--15},
title = {{Natural Language Inference over Interaction Space}},
url = {http://arxiv.org/abs/1709.04348},
year = {2017}
}

@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
pages = {1--9},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
